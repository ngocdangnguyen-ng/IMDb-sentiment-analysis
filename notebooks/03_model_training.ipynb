{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe0c231",
   "metadata": {},
   "source": [
    "# Model Training - IMDb Movie Reviews Sentiment Analysis \n",
    "## Introduction\n",
    "This notebook focuses on **model training and comparison** for the IMDb Movie Reviews sentiment analysis, building upon the preprocessed data. We'll implement and compare both traditional machine learning and deep learning approaches to find the best performing model.\n",
    "\n",
    "**Dataset:** IMDB Dataset of 50K Movie Reviews (Kaggle)\n",
    "\n",
    "**Objective:** Train, evaluate, and compare various ML models for sentiment classification\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen - Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Setup and load preprocessed data\n",
    "2. Traditional ML models (Logistic Regression, Naive Bayes, Random Forest, SVM)\n",
    "3. Deep Learning models (LSTM, CNN, Bidirectional LSTM)\n",
    "4. Advanced approaches (Transfer Learning with pre-trained embeddings)\n",
    "5. Model comparison and performance analysis\n",
    "6. Hyperparameter tuning for best models\n",
    "7. Final model selection and saving\n",
    "8. Deployment preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c3350",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c941c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# TensorFlow/Keras - compatible import\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from keras.utils import to_categorical\n",
    "    print(\"Using standalone Keras\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        print(\"Using tensorflow.keras\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import Keras modules\")\n",
    "\n",
    "# Import from our modules\n",
    "from config import *\n",
    "\n",
    "# Create directories for saving results\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "os.makedirs('results/reports', exist_ok=True)\n",
    "os.makedirs('models/saved_models', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95d62c",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb69782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load traditional ML data\n",
    "ml_data = np.load('data/processed/traditional_ml_data.npz')\n",
    "X_train_tfidf = ml_data['X_train']\n",
    "X_val_tfidf = ml_data['X_val']\n",
    "X_test_tfidf = ml_data['X_test']\n",
    "y_train = ml_data['y_train']\n",
    "y_val = ml_data['y_val']\n",
    "y_test = ml_data['y_test']\n",
    "\n",
    "print(f\"Traditional ML data loaded:\")\n",
    "print(f\"  Train: {X_train_tfidf.shape}, Val: {X_val_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Load deep learning data\n",
    "dl_data = np.load('data/processed/deep_learning_data.npz')\n",
    "X_train_seq = dl_data['X_train']\n",
    "X_val_seq = dl_data['X_val']\n",
    "X_test_seq = dl_data['X_test']\n",
    "\n",
    "print(f\"Deep Learning data loaded:\")\n",
    "print(f\"  Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}\")\n",
    "\n",
    "# Load preprocessing config\n",
    "with open('models/preprocessors/preprocessing_config.pkl', 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = config['max_sequence_length']\n",
    "VOCAB_SIZE = config['vocab_size']\n",
    "\n",
    "print(f\"Config loaded: Max sequence length: {MAX_SEQUENCE_LENGTH}, Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Train: {np.bincount(y_train)} ({np.bincount(y_train)/len(y_train)*100}%)\")\n",
    "print(f\"Val: {np.bincount(y_val)} ({np.bincount(y_val)/len(y_val)*100}%)\")\n",
    "print(f\"Test: {np.bincount(y_test)} ({np.bincount(y_test)/len(y_test)*100}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3300e",
   "metadata": {},
   "source": [
    "## 3. Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRADITIONAL MACHINE LEARNING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clear any existing model_results to avoid duplicates from re-running\n",
    "model_results = {}\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation function\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get prediction probabilities for ROC-AUC (if available)\n",
    "    try:\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    except:\n",
    "        # For models without predict_proba (like SVM with linear kernel)\n",
    "        y_train_proba = model.decision_function(X_train) if hasattr(model, 'decision_function') else None\n",
    "        y_val_proba = model.decision_function(X_val) if hasattr(model, 'decision_function') else None\n",
    "        y_test_proba = model.decision_function(X_test) if hasattr(model, 'decision_function') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'training_time': training_time,\n",
    "        \n",
    "        # Accuracy\n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
    "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        \n",
    "        # Precision, Recall, F1\n",
    "        'train_precision': precision_score(y_train, y_train_pred),\n",
    "        'val_precision': precision_score(y_val, y_val_pred),\n",
    "        'test_precision': precision_score(y_test, y_test_pred),\n",
    "        \n",
    "        'train_recall': recall_score(y_train, y_train_pred),\n",
    "        'val_recall': recall_score(y_val, y_val_pred),\n",
    "        'test_recall': recall_score(y_test, y_test_pred),\n",
    "        \n",
    "        'train_f1': f1_score(y_train, y_train_pred),\n",
    "        'val_f1': f1_score(y_val, y_val_pred),\n",
    "        'test_f1': f1_score(y_test, y_test_pred),\n",
    "    }\n",
    "    \n",
    "    # ROC-AUC if probabilities available\n",
    "    if y_val_proba is not None:\n",
    "        metrics.update({\n",
    "            'train_roc_auc': roc_auc_score(y_train, y_train_proba),\n",
    "            'val_roc_auc': roc_auc_score(y_val, y_val_proba),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_test_proba),\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Val Accuracy: {metrics['val_accuracy']:.4f}\")\n",
    "    print(f\"  Val F1-Score: {metrics['val_f1']:.4f}\")\n",
    "    if 'val_roc_auc' in metrics:\n",
    "        print(f\"  Val ROC-AUC: {metrics['val_roc_auc']:.4f}\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"1. Logistic Regression\")\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    C=1.0\n",
    ")\n",
    "lr_trained, lr_metrics = evaluate_model(\n",
    "    lr_model, X_train_tfidf, X_val_tfidf, X_test_tfidf, \n",
    "    y_train, y_val, y_test, \"Logistic Regression\"\n",
    ")\n",
    "model_results['Logistic Regression'] = lr_metrics\n",
    "\n",
    "# 2. Multinomial Naive Bayes\n",
    "print(\"\\n2. Multinomial Naive Bayes\")\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_trained, nb_metrics = evaluate_model(\n",
    "    nb_model, X_train_tfidf, X_val_tfidf, X_test_tfidf,\n",
    "    y_train, y_val, y_test, \"Naive Bayes\"\n",
    ")\n",
    "model_results['Naive Bayes'] = nb_metrics\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3. Random Forest\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_trained, rf_metrics = evaluate_model(\n",
    "    rf_model, X_train_tfidf, X_val_tfidf, X_test_tfidf,\n",
    "    y_train, y_val, y_test, \"Random Forest\"\n",
    ")\n",
    "model_results['Random Forest'] = rf_metrics\n",
    "\n",
    "# Import LinearSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# 4. Support Vector Machine (Linear) \n",
    "print(\"\\n4. Support Vector Machine (LinearSVC)\")\n",
    "linear_svm_model = LinearSVC(\n",
    "    C=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=2000,  # Increased maximum iterations\n",
    "    dual=False      # Faster when n_samples > n_features\n",
    ")\n",
    "\n",
    "try:\n",
    "    # If probabilities are needed, CalibratedClassifierCV can be used to wrap LinearSVC\n",
    "    if False:  # Set to True if you really need probabilities\n",
    "        calibrated_svm = CalibratedClassifierCV(linear_svm_model, cv=3)\n",
    "        linear_svm_trained, linear_svm_metrics = evaluate_model(\n",
    "            calibrated_svm, X_train_tfidf, X_val_tfidf, X_test_tfidf,\n",
    "            y_train, y_val, y_test, \"LinearSVC (Calibrated)\"\n",
    "        )\n",
    "    else:\n",
    "        linear_svm_trained, linear_svm_metrics = evaluate_model(\n",
    "            linear_svm_model, X_train_tfidf, X_val_tfidf, X_test_tfidf,\n",
    "            y_train, y_val, y_test, \"LinearSVC\"\n",
    "        )\n",
    "    \n",
    "    model_results['LinearSVC'] = linear_svm_metrics\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {e}\")\n",
    "    print(\"Please run cells from the beginning to ensure all data has been loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a79dce",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449fdf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDEEP LEARNING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Common training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def train_deep_model(model, model_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Train deep learning model with callbacks\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Model checkpoint\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f'models/saved_models/{model_name.lower().replace(\" \", \"_\")}_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    # Get predictions for detailed metrics\n",
    "    y_train_pred_proba = model.predict(X_train, verbose=0)\n",
    "    y_val_pred_proba = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    y_train_pred = (y_train_pred_proba > 0.5).astype(int).flatten()\n",
    "    y_val_pred = (y_val_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'training_time': training_time,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        \n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
    "        \n",
    "        'train_precision': precision_score(y_train, y_train_pred),\n",
    "        'val_precision': precision_score(y_val, y_val_pred),\n",
    "        \n",
    "        'train_recall': recall_score(y_train, y_train_pred),\n",
    "        'val_recall': recall_score(y_val, y_val_pred),\n",
    "        \n",
    "        'train_f1': f1_score(y_train, y_train_pred),\n",
    "        'val_f1': f1_score(y_val, y_val_pred),\n",
    "        \n",
    "        'train_roc_auc': roc_auc_score(y_train, y_train_pred_proba.flatten()),\n",
    "        'val_roc_auc': roc_auc_score(y_val, y_val_pred_proba.flatten()),\n",
    "        \n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Epochs trained: {len(history.history['loss'])}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Val F1-Score: {metrics['val_f1']:.4f}\")\n",
    "    print(f\"  Val ROC-AUC: {metrics['val_roc_auc']:.4f}\")\n",
    "    \n",
    "    return model, history, metrics\n",
    "\n",
    "# 1. Simple LSTM Model\n",
    "print(\"1. LSTM Model\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_trained, lstm_history, lstm_metrics = train_deep_model(\n",
    "    lstm_model, \"LSTM\", X_train_seq, X_val_seq, y_train, y_val\n",
    ")\n",
    "model_results['LSTM'] = lstm_metrics\n",
    "\n",
    "# 2. CNN Model\n",
    "print(\"\\n2. CNN Model\")\n",
    "cnn_model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_trained, cnn_history, cnn_metrics = train_deep_model(\n",
    "    cnn_model, \"CNN\", X_train_seq, X_val_seq, y_train, y_val\n",
    ")\n",
    "model_results['CNN'] = cnn_metrics\n",
    "\n",
    "# 3. Bidirectional LSTM Model\n",
    "print(\"\\n3. Bidirectional LSTM Model\")\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "bilstm_model.summary()\n",
    "\n",
    "bilstm_trained, bilstm_history, bilstm_metrics = train_deep_model(\n",
    "    bilstm_model, \"Bidirectional LSTM\", X_train_seq, X_val_seq, y_train, y_val\n",
    ")\n",
    "model_results['Bidirectional LSTM'] = bilstm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaed994",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4865fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMODEL COMPARISON AND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "\n",
    "# Select key metrics for comparison\n",
    "key_metrics = ['val_accuracy', 'val_f1', 'val_roc_auc', 'training_time']\n",
    "comparison_subset = comparison_df[key_metrics].copy()\n",
    "\n",
    "# Convert data type to float for metric columns\n",
    "for col in key_metrics:\n",
    "    comparison_subset[col] = pd.to_numeric(comparison_subset[col], errors='coerce')\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_subset.round(4))\n",
    "\n",
    "# Find best model by validation accuracy\n",
    "best_model_name = comparison_subset['val_accuracy'].idxmax()\n",
    "print(f\"\\nBest model by validation accuracy: {best_model_name}\")\n",
    "print(f\"Best validation accuracy: {comparison_subset.loc[best_model_name, 'val_accuracy']:.4f}\")\n",
    "\n",
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[0,0].bar(comparison_subset.index, comparison_subset['val_accuracy'], color='skyblue')\n",
    "axes[0,0].set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Validation F1-Score\n",
    "axes[0,1].bar(comparison_subset.index, comparison_subset['val_f1'], color='lightgreen')\n",
    "axes[0,1].set_title('Validation F1-Score Comparison', fontweight='bold')\n",
    "axes[0,1].set_ylabel('F1-Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Validation ROC-AUC\n",
    "valid_roc_auc = comparison_subset['val_roc_auc'].dropna()\n",
    "axes[1,0].bar(valid_roc_auc.index, valid_roc_auc.values, color='lightcoral')\n",
    "axes[1,0].set_title('Validation ROC-AUC Comparison', fontweight='bold')\n",
    "axes[1,0].set_ylabel('ROC-AUC')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Training Time\n",
    "axes[1,1].bar(comparison_subset.index, comparison_subset['training_time'], color='orange')\n",
    "axes[1,1].set_title('Training Time Comparison', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Time (seconds)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Performance vs Training Time scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name in comparison_subset.index:\n",
    "    plt.scatter(\n",
    "        comparison_subset.loc[model_name, 'training_time'],\n",
    "        comparison_subset.loc[model_name, 'val_accuracy'],\n",
    "        s=100, label=model_name\n",
    "    )\n",
    "    \n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Model Performance vs Training Time', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('results/plots/performance_vs_time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29401b9",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis of Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDETAILED ANALYSIS OF BEST MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ensure numeric data type for comparison_subset\n",
    "for col in comparison_subset.columns:\n",
    "    comparison_subset[col] = pd.to_numeric(comparison_subset[col], errors='coerce')\n",
    "\n",
    "# Select top 3 models for detailed analysis\n",
    "top_3_models = comparison_subset.nlargest(3, 'val_accuracy')\n",
    "print(\"Top 3 models by validation accuracy:\")\n",
    "print(top_3_models[['val_accuracy', 'val_f1', 'training_time']])\n",
    "\n",
    "# Learning curves for deep learning models (if available)\n",
    "if 'lstm_history' in locals():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # LSTM learning curves\n",
    "    axes[0].plot(lstm_history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[0].plot(lstm_history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    axes[0].set_title('LSTM Learning Curves', fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # CNN learning curves\n",
    "    axes[1].plot(cnn_history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[1].plot(cnn_history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    axes[1].set_title('CNN Learning Curves', fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # BiLSTM learning curves\n",
    "    axes[2].plot(bilstm_history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[2].plot(bilstm_history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "    axes[2].set_title('Bidirectional LSTM Learning Curves', fontweight='bold')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/plots/learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e3d65",
   "metadata": {},
   "source": [
    "## 7. Final Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check available models\n",
    "print(\"Available models:\", list(model_results.keys()))\n",
    "\n",
    "# Evaluate best traditional ML model on test set\n",
    "available_traditional = [model for model in ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'Linear SVM'] \n",
    "                        if model in model_results.keys()]\n",
    "\n",
    "if available_traditional:\n",
    "    best_traditional = max(available_traditional, key=lambda x: model_results[x]['val_accuracy'])\n",
    "    print(f\"Best Traditional ML Model: {best_traditional}\")\n",
    "    \n",
    "    # Get the trained model and evaluate on test set\n",
    "    if best_traditional == 'Logistic Regression':\n",
    "        best_trad_model = lr_trained\n",
    "    elif best_traditional == 'Naive Bayes':\n",
    "        best_trad_model = nb_trained\n",
    "    elif best_traditional == 'Random Forest':\n",
    "        best_trad_model = rf_trained\n",
    "    elif best_traditional == 'Linear SVM':\n",
    "        best_trad_model = linear_svm_trained\n",
    "    \n",
    "    # Test predictions\n",
    "    y_test_pred_trad = best_trad_model.predict(X_test_tfidf)\n",
    "    test_acc_trad = accuracy_score(y_test, y_test_pred_trad)\n",
    "    test_f1_trad = f1_score(y_test, y_test_pred_trad)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_acc_trad:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1_trad:.4f}\")\n",
    "    \n",
    "    # Traditional ML confusion matrix\n",
    "    cm_trad = confusion_matrix(y_test, y_test_pred_trad)\n",
    "    print(\"Traditional ML Confusion Matrix:\")\n",
    "    print(cm_trad)\n",
    "else:\n",
    "    print(\"No traditional ML models found!\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Deep Learning models\n",
    "available_dl = [model for model in ['LSTM', 'BiLSTM', 'CNN'] \n",
    "               if model in model_results.keys()]\n",
    "\n",
    "if available_dl:\n",
    "    best_dl = max(available_dl, key=lambda x: model_results[x]['val_accuracy'])\n",
    "    print(f\"Best Deep Learning Model: {best_dl}\")\n",
    "    \n",
    "    # Get the trained model\n",
    "    if best_dl == 'LSTM':\n",
    "        best_dl_model = lstm_trained\n",
    "    elif best_dl == 'BiLSTM':\n",
    "        best_dl_model = bilstm_trained\n",
    "    elif best_dl == 'CNN':\n",
    "        best_dl_model = cnn_trained\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_loss_dl, test_acc_dl = best_dl_model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "    y_test_pred_dl_proba = best_dl_model.predict(X_test_seq)\n",
    "    y_test_pred_dl = (y_test_pred_dl_proba > 0.5).astype(int)\n",
    "    test_f1_dl = f1_score(y_test, y_test_pred_dl)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss_dl:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc_dl:.4f}\")  \n",
    "    print(f\"Test F1 Score: {test_f1_dl:.4f}\")\n",
    "    \n",
    "    # Deep Learning confusion matrix\n",
    "    cm_dl = confusion_matrix(y_test, y_test_pred_dl)\n",
    "    print(\"Deep Learning Confusion Matrix:\")\n",
    "    print(cm_dl)\n",
    "else:\n",
    "    print(\"No deep learning models found!\")\n",
    "\n",
    "# Overall comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OVERALL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find overall best model\n",
    "overall_best = None\n",
    "overall_best_acc = 0\n",
    "\n",
    "for model_name, metrics in model_results.items():\n",
    "    if metrics['val_accuracy'] > overall_best_acc:\n",
    "        overall_best_acc = metrics['val_accuracy']\n",
    "        overall_best = model_name\n",
    "\n",
    "print(f\"Overall Best Model: {overall_best}\")\n",
    "print(f\"Best Validation Accuracy: {overall_best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98d964",
   "metadata": {},
   "source": [
    "## 8. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSAVING MODELS AND RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save traditional ML models\n",
    "with open(f'models/saved_models/{best_traditional.lower().replace(\" \", \"_\")}.pkl', 'wb') as f:\n",
    "    pickle.dump(best_trad_model, f)\n",
    "\n",
    "# Deep learning models are already saved via ModelCheckpoint callback\n",
    "\n",
    "# Save comprehensive results\n",
    "final_results = {\n",
    "    'model_comparison': comparison_df.to_dict(),\n",
    "    'best_traditional_model': best_traditional,\n",
    "    'best_deep_learning_model': best_dl,\n",
    "    'overall_best_model': overall_best,\n",
    "    'test_results': {\n",
    "        'traditional_ml': {\n",
    "            'model': best_traditional,\n",
    "            'test_accuracy': test_acc_trad,\n",
    "            'test_f1': test_f1_trad\n",
    "        },\n",
    "        'deep_learning': {\n",
    "            'model': best_dl,\n",
    "            'test_accuracy': test_acc_dl,\n",
    "            'test_f1': test_f1_dl\n",
    "        }\n",
    "    },\n",
    "    'training_config': {\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'embedding_dim': EMBEDDING_DIM\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('results/reports/training_results.pkl', 'wb') as f:\n",
    "    pickle.dump(final_results, f)\n",
    "\n",
    "# Save results as CSV for easy viewing\n",
    "comparison_df.round(4).to_csv('results/reports/model_comparison.csv')\n",
    "\n",
    "# Generate detailed report\n",
    "report_text = f\"\"\"\n",
    "IMDb Sentiment Analysis - Model Training Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "=== DATASET SUMMARY ===\n",
    "Training samples: {len(y_train):,}\n",
    "Validation samples: {len(y_val):,}\n",
    "Test samples: {len(y_test):,}\n",
    "Features (TF-IDF): {X_train_tfidf.shape[1]:,}\n",
    "Max sequence length: {MAX_SEQUENCE_LENGTH}\n",
    "Vocabulary size: {VOCAB_SIZE:,}\n",
    "\n",
    "=== MODEL PERFORMANCE SUMMARY ===\n",
    "\"\"\"\n",
    "\n",
    "for model_name in model_results:\n",
    "    result = model_results[model_name]\n",
    "    report_text += f\"\"\"\n",
    "{model_name}:\n",
    "  Validation Accuracy: {result['val_accuracy']:.4f}\n",
    "  Validation F1-Score: {result['val_f1']:.4f}\n",
    "  Training Time: {result['training_time']:.2f}s\n",
    "\"\"\"\n",
    "\n",
    "report_text += f\"\"\"\n",
    "=== BEST MODEL SELECTION ===\n",
    "Best Traditional ML: {best_traditional} (Val Acc: {model_results[best_traditional]['val_accuracy']:.4f})\n",
    "Best Deep Learning: {best_dl} (Val Acc: {model_results[best_dl]['val_accuracy']:.4f})\n",
    "Overall Best: {overall_best} (Test Acc: {overall_best_acc:.4f})\n",
    "\n",
    "=== TEST SET EVALUATION ===\n",
    "{best_traditional} Test Accuracy: {test_acc_trad:.4f}\n",
    "{best_dl} Test Accuracy: {test_acc_dl:.4f}\n",
    "\n",
    "=== RECOMMENDATIONS ===\n",
    "1. The {overall_best} model achieved the highest test accuracy of {overall_best_acc:.4f}\n",
    "2. Consider ensemble methods combining traditional ML and deep learning\n",
    "3. Experiment with pre-trained embeddings (GloVe, Word2Vec) for potential improvements\n",
    "4. Hyperparameter tuning could further improve performance\n",
    "5. For production deployment, consider model size vs accuracy trade-offs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8012e0",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAVING MODELS FOR WEB APP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create directory for saved models\n",
    "import os\n",
    "os.makedirs('../models/saved_models', exist_ok=True)\n",
    "\n",
    "# Find best traditional ML model\n",
    "print(\"Finding best traditional ML model...\")\n",
    "traditional_models = {name: metrics for name, metrics in model_results.items() \n",
    "                     if name in ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'LinearSVC']}\n",
    "\n",
    "if traditional_models:\n",
    "    best_trad_name = max(traditional_models.keys(), key=lambda x: traditional_models[x]['val_accuracy'])\n",
    "    print(f\"Best traditional model: {best_trad_name} ({traditional_models[best_trad_name]['val_accuracy']:.4f})\")\n",
    "    \n",
    "    # Get the trained model\n",
    "    if best_trad_name == 'Logistic Regression':\n",
    "        best_trad_model = lr_trained\n",
    "    elif best_trad_name == 'Naive Bayes':\n",
    "        best_trad_model = nb_trained\n",
    "    elif best_trad_name == 'Random Forest':\n",
    "        best_trad_model = rf_trained\n",
    "    elif best_trad_name == 'LinearSVC':\n",
    "        best_trad_model = linear_svm_trained\n",
    "    \n",
    "    # Save best traditional model\n",
    "    with open('../models/saved_models/best_traditional_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_trad_model, f)\n",
    "    print(f\"✓ Saved {best_trad_name} model\")\n",
    "\n",
    "# Find best deep learning model\n",
    "print(\"\\nFinding best deep learning model...\")\n",
    "dl_models = {name: metrics for name, metrics in model_results.items() \n",
    "            if name in ['LSTM', 'CNN', 'Bidirectional LSTM']}\n",
    "\n",
    "if dl_models:\n",
    "    best_dl_name = max(dl_models.keys(), key=lambda x: dl_models[x]['val_accuracy'])\n",
    "    print(f\"Best deep learning model: {best_dl_name} ({dl_models[best_dl_name]['val_accuracy']:.4f})\")\n",
    "    \n",
    "    # Get the trained model\n",
    "    if best_dl_name == 'LSTM':\n",
    "        best_dl_model = lstm_trained\n",
    "    elif best_dl_name == 'CNN':\n",
    "        best_dl_model = cnn_trained\n",
    "    elif best_dl_name == 'Bidirectional LSTM':\n",
    "        best_dl_model = bilstm_trained\n",
    "    \n",
    "    # Save best deep learning model\n",
    "    best_dl_model.save('../models/saved_models/best_dl_model.h5')\n",
    "    print(f\"✓ Saved {best_dl_name} model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"You can now run the web app with:\")\n",
    "print(\"  cd app\")\n",
    "print(\"  python app.py\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ebb6bb",
   "metadata": {},
   "source": [
    "## Model Training Conclusion\n",
    "Successfully trained and compared multiple ML approaches for sentiment analysis, achieving strong performance across both traditional and deep learning models. Traditional ML delivered excellent results with Logistic Regression (89.66% validation accuracy) and Linear SVM (88.66%), while deep learning models showed competitive performance with CNN architecture leading at 90.34% validation accuracy.\n",
    "\n",
    "The comprehensive comparison revealed that both approaches are highly effective for this sentiment classification task, with deep learning providing marginal improvement over traditional methods. All best-performing models have been automatically saved and are ready for deployment in the web application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
