{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0bc284",
   "metadata": {},
   "source": [
    "# Data Preprocessing - IMDb Movie Reviews Sentiment Analysis\n",
    "## Introduction\n",
    "This notebook focuses on **text preprocessing** for the IMDb Movie Reviews dataset, building upon the insights from the exploratory data analysis. The goal is to clean and transform the raw text data into formats suitable for machine learning models.\n",
    "\n",
    "**Dataset:** IMDB Dataset of 50K Movie Reviews (Kaggle)\n",
    "\n",
    "**Objective:** Clean, transform, and prepare text data for both traditional ML and deep learning approaches\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen - Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "1. Load cleaned dataset and setup\n",
    "2. Text cleaning functions implementation\n",
    "3. Basic text preprocessing (HTML removal, normalization)\n",
    "4. Advanced text preprocessing (tokenization, stopword removal)\n",
    "5. Feature extraction for traditional ML (TF-IDF, Count Vectorizer)\n",
    "6. Text preparation for deep learning (tokenization, padding)\n",
    "7. Train/Validation/Test split with stratification\n",
    "8. Save processed data and preprocessing artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439a18a",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TensorFlow/Keras - compatible import\n",
    "try:\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    print(\"Using standalone Keras\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        print(\"Using tensorflow.keras\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import Keras preprocessing modules\")\n",
    "\n",
    "# Import from our modules\n",
    "from config import *\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6e25b",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "df = pd.read_csv('../data/raw/IMDB Dataset.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Quick overview\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Create binary labels for machine learning\n",
    "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(f\"\\nLabel mapping: positive=1, negative=0\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51faaf",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning Functions Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text using BeautifulSoup\"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    cleaned = soup.get_text()\n",
    "    return cleaned\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "def remove_special_chars(text, keep_punctuation=False):\n",
    "    \"\"\"Remove special characters, optionally keep basic punctuation\"\"\"\n",
    "    if keep_punctuation:\n",
    "        # Keep basic punctuation: . , ! ? ; :\n",
    "        pattern = r'[^a-zA-Z0-9\\s.,!?;:\\'-]'\n",
    "    else:\n",
    "        # Remove all special characters except spaces\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Basic text normalization\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"Remove standalone numbers\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand common English contractions\"\"\"\n",
    "    contractions_dict = {\n",
    "        \"ain't\": \"are not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "        \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\", \"weren't\": \"were not\", \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\", \"who's\": \"who is\", \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def basic_text_cleaning(text):\n",
    "    \"\"\"Apply basic cleaning pipeline\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    # Remove URLs\n",
    "    text = remove_urls(text)\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    # Remove special characters (keep basic punctuation)\n",
    "    text = remove_special_chars(text, keep_punctuation=True)\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"Text cleaning functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673c8a1",
   "metadata": {},
   "source": [
    "## 4. Apply Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLYING BASIC TEXT PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Apply basic cleaning\n",
    "print(\"Step 1: Basic text cleaning...\")\n",
    "df['cleaned_text'] = df['review'].apply(basic_text_cleaning)\n",
    "\n",
    "# Show examples of cleaning results\n",
    "print(\"Cleaning examples:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Original:\")\n",
    "    print(df.iloc[i]['review'][:200] + \"...\")\n",
    "    print(\"Cleaned:\")\n",
    "    print(df.iloc[i]['cleaned_text'][:200] + \"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare lengths before and after cleaning\n",
    "df['original_length'] = df['review'].str.len()\n",
    "df['cleaned_length'] = df['cleaned_text'].str.len()\n",
    "\n",
    "reduction_ratio = ((df['original_length'] - df['cleaned_length']) / df['original_length'] * 100).mean()\n",
    "print(f\"\\nAverage text length reduction: {reduction_ratio:.1f}%\")\n",
    "print(f\"Original average length: {df['original_length'].mean():.0f} characters\")\n",
    "print(f\"Cleaned average length: {df['cleaned_length'].mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2340d",
   "metadata": {},
   "source": [
    "## 5. Advanced Text Preprocessing for Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nADVANCED PREPROCESSING FOR TRADITIONAL ML\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Number of English stopwords: {len(stop_words)}\")\n",
    "\n",
    "def advanced_preprocessing_ml(text, remove_stopwords=True, apply_stemming=False, apply_lemmatization=True):\n",
    "    \"\"\"Advanced preprocessing for traditional ML models\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove non-alphabetic tokens and short words\n",
    "    tokens = [token for token in tokens if token.isalpha() and len(token) > 2]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    if apply_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif apply_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Create different preprocessing versions for comparison\n",
    "print(\"Creating different preprocessing versions...\")\n",
    "\n",
    "# Version 1: Basic cleaning + stopword removal + lemmatization\n",
    "print(\"Processing ML Version 1: Basic + Stopwords removal + Lemmatization...\")\n",
    "df['text_ml_v1'] = df['cleaned_text'].apply(\n",
    "    lambda x: advanced_preprocessing_ml(x, remove_stopwords=True, apply_lemmatization=True)\n",
    ")\n",
    "\n",
    "# Version 2: Basic cleaning + stemming (no stopword removal for comparison)\n",
    "print(\"Processing ML Version 2: Basic + Stemming (keeping stopwords)...\")\n",
    "df['text_ml_v2'] = df['cleaned_text'].apply(\n",
    "    lambda x: advanced_preprocessing_ml(x, remove_stopwords=False, apply_stemming=True)\n",
    ")\n",
    "\n",
    "# Version 3: Minimal preprocessing (just basic cleaning)\n",
    "print(\"Processing ML Version 3: Minimal (just basic cleaning)...\")\n",
    "df['text_ml_minimal'] = df['cleaned_text']\n",
    "\n",
    "# Compare word counts across versions\n",
    "df['word_count_v1'] = df['text_ml_v1'].str.split().str.len()\n",
    "df['word_count_v2'] = df['text_ml_v2'].str.split().str.len()\n",
    "df['word_count_minimal'] = df['text_ml_minimal'].str.split().str.len()\n",
    "\n",
    "# Statistics comparison\n",
    "print(\"\\nWord count statistics across preprocessing versions:\")\n",
    "versions_stats = pd.DataFrame({\n",
    "    'Original': df['review'].str.split().str.len().describe(),\n",
    "    'V1 (Lem+Stop)': df['word_count_v1'].describe(),\n",
    "    'V2 (Stem)': df['word_count_v2'].describe(),\n",
    "    'Minimal': df['word_count_minimal'].describe()\n",
    "}).round(2)\n",
    "print(versions_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf1244",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction for Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeefcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFEATURE EXTRACTION FOR TRADITIONAL ML\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# We'll use the best version (V1) for feature extraction\n",
    "text_for_ml = df['text_ml_v1']\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,      # Top 10K most important features\n",
    "    ngram_range=(1, 2),      # Unigrams and bigrams\n",
    "    min_df=2,                # Ignore terms appearing in less than 2 documents\n",
    "    max_df=0.95,             # Ignore terms appearing in more than 95% of documents\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_for_ml)\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Matrix density: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")\n",
    "\n",
    "# Count Vectorization\n",
    "print(\"\\nCreating Count Vectorizer features...\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(text_for_ml)\n",
    "print(f\"Count matrix shape: {count_matrix.shape}\")\n",
    "\n",
    "# Show top features\n",
    "print(\"\\nTop 20 TF-IDF features:\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
    "top_features = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "for feature, score in top_features:\n",
    "    print(f\"{feature}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85585229",
   "metadata": {},
   "source": [
    "## 7. Text Preparation for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTEXT PREPARATION FOR DEEP LEARNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# For deep learning, we use less aggressive preprocessing\n",
    "def preprocessing_for_dl(text):\n",
    "    \"\"\"Preprocessing for deep learning models (less aggressive)\"\"\"\n",
    "    # Basic cleaning (already done)\n",
    "    # Keep original structure more intact\n",
    "    text = text.lower()\n",
    "    # Remove numbers but keep text structure\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Prepare text for deep learning\n",
    "print(\"Processing text for deep learning...\")\n",
    "df['text_dl'] = df['cleaned_text'].apply(preprocessing_for_dl)\n",
    "\n",
    "# Tokenization for deep learning\n",
    "print(\"Creating tokenizer for deep learning...\")\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=20000,          # Vocabulary size\n",
    "    oov_token='<OOV>',        # Out of vocabulary token\n",
    "    lower=True,\n",
    "    split=' '\n",
    ")\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer.fit_on_texts(df['text_dl'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['text_dl'])\n",
    "\n",
    "# Analyze sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "seq_length_stats = pd.Series(sequence_lengths).describe()\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(seq_length_stats)\n",
    "\n",
    "# Visualize sequence length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sequence_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(sequence_lengths), linestyle='--', label=f'Mean: {np.mean(sequence_lengths):.0f}')\n",
    "plt.axvline(np.percentile(sequence_lengths, 90), linestyle='--', label=f'90th percentile: {np.percentile(sequence_lengths, 90):.0f}')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title('Box Plot of Sequence Lengths')\n",
    "plt.ylabel('Sequence Length')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose max sequence length (capture 90% of data)\n",
    "MAX_SEQUENCE_LENGTH = int(np.percentile(sequence_lengths, 90))\n",
    "print(f\"\\nChosen MAX_SEQUENCE_LENGTH: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Pad sequences\n",
    "print(\"Padding sequences...\")\n",
    "padded_sequences = pad_sequences(sequences, \n",
    "                                maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                                padding='post', \n",
    "                                truncating='post')\n",
    "\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
    "\n",
    "# Vocabulary info\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Using top {min(vocab_size, 20000)} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53ed2c",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "X_train_text, X_temp_text, y_train, y_temp = train_test_split(\n",
    "    df['text_ml_v1'], df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Second split: 10% validation, 10% test from the 20% temp\n",
    "X_val_text, X_test_text, y_val, y_test = train_test_split(\n",
    "    X_temp_text, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_text)} ({len(X_train_text)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val_text)} ({len(X_val_text)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test_text)} ({len(X_test_text)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check stratification\n",
    "print(\"\\nClass distribution check:\")\n",
    "print(\"Training set:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Validation set:\")\n",
    "print(pd.Series(y_val).value_counts(normalize=True))\n",
    "print(\"Test set:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True))\n",
    "\n",
    "# Apply TF-IDF transformation to splits\n",
    "print(\"\\nApplying TF-IDF transformation to splits...\")\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"Train TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation TF-IDF shape: {X_val_tfidf.shape}\")\n",
    "print(f\"Test TF-IDF shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Prepare deep learning data splits\n",
    "# Get indices for the splits\n",
    "train_indices = X_train_text.index\n",
    "val_indices = X_val_text.index\n",
    "test_indices = X_test_text.index\n",
    "\n",
    "# Extract corresponding padded sequences\n",
    "X_train_seq = padded_sequences[train_indices]\n",
    "X_val_seq = padded_sequences[val_indices]\n",
    "X_test_seq = padded_sequences[test_indices]\n",
    "\n",
    "print(f\"\\nDeep Learning data shapes:\")\n",
    "print(f\"Train sequences: {X_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}\")\n",
    "print(f\"Test sequences: {X_test_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db84b7",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data and Preprocessing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSAVING PROCESSED DATA AND ARTIFACTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../models/preprocessors', exist_ok=True)\n",
    "\n",
    "# Save processed datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Traditional ML data\n",
    "np.savez_compressed('../data/processed/traditional_ml_data.npz',\n",
    "                    X_train=X_train_tfidf.toarray(),\n",
    "                    X_val=X_val_tfidf.toarray(),\n",
    "                    X_test=X_test_tfidf.toarray(),\n",
    "                    y_train=y_train.values,\n",
    "                    y_val=y_val.values,\n",
    "                    y_test=y_test.values)\n",
    "\n",
    "# Deep learning data\n",
    "np.savez_compressed('../data/processed/deep_learning_data.npz',\n",
    "                    X_train=X_train_seq,\n",
    "                    X_val=X_val_seq,\n",
    "                    X_test=X_test_seq,\n",
    "                    y_train=y_train.values,\n",
    "                    y_val=y_val.values,\n",
    "                    y_test=y_test.values)\n",
    "\n",
    "# Save text data for reference\n",
    "pd.DataFrame({\n",
    "    'text': X_train_text,\n",
    "    'label': y_train\n",
    "}).to_csv('../data/processed/train_text.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'text': X_val_text,\n",
    "    'label': y_val\n",
    "}).to_csv('../data/processed/val_text.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'text': X_test_text,\n",
    "    'label': y_test\n",
    "}).to_csv('../data/processed/test_text.csv', index=False)\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "print(\"Saving preprocessing artifacts...\")\n",
    "\n",
    "# Save vectorizers\n",
    "with open('../models/preprocessors/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('../models/preprocessors/count_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "\n",
    "# Save tokenizer\n",
    "with open('../models/preprocessors/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save preprocessing parameters\n",
    "preprocessing_config = {\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'vocab_size': vocab_size,\n",
    "    'tfidf_max_features': 10000,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'test_size': 0.2,\n",
    "    'val_split': 0.5\n",
    "}\n",
    "\n",
    "with open('../models/preprocessors/preprocessing_config.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_config, f)\n",
    "\n",
    "# Save the full preprocessed dataframe for reference\n",
    "print(\"Saving full preprocessed dataframe...\")\n",
    "df_processed = df[[\n",
    "    'review', 'sentiment', 'label', \n",
    "    'cleaned_text', 'text_ml_v1', 'text_dl'\n",
    "]].copy()\n",
    "df_processed.to_csv('../data/processed/full_preprocessed_data.csv', index=False)\n",
    "\n",
    "print(\"All data and artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c15a4",
   "metadata": {},
   "source": [
    "## Data Preprocessing Conclusion\n",
    "Successfully implemented comprehensive text preprocessing pipeline for IMDb reviews, creating clean datasets for both traditional ML and deep learning approaches. Key achievements include HTML tag removal, contraction expansion, advanced tokenization, and strategic feature extraction using TF-IDF (10K features) and sequence padding (90th percentile length).\n",
    "\n",
    "The preprocessing pipeline generated stratified train/validation/test splits (80%/10%/10%) with balanced class distribution. Traditional ML data uses lemmatization and stopword removal, while deep learning data preserves text structure with vocabulary size of 20K tokens. All processed datasets and preprocessing artifacts saved successfully for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
